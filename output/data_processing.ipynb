{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'simulations'\n",
    "simulation_folders = os.listdir(root_dir)\n",
    "dataframes = {}\n",
    "\n",
    "for folder in simulation_folders:\n",
    "    folder_path = os.path.join(root_dir, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "        \n",
    "        metrics = []\n",
    "        for json_file in json_files:\n",
    "            json_file_path = os.path.join(folder_path, json_file)\n",
    "            \n",
    "            with open(json_file_path, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            items = json_data['data']['items']\n",
    "\n",
    "            metric_data = {item['title']: item['value'] for item in items if item['title'] != 'Time to discovery'}\n",
    "            metrics.append(metric_data)\n",
    "\n",
    "        df = pd.DataFrame(metrics)\n",
    "        dataframes[folder] = df\n",
    "\n",
    "# Access the DataFrame for a specific simulation folder like this:\n",
    "# dataframes['Appenzeller-Herzog_2019_-m_logistic_-e_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(df):\n",
    "    metric_averages = {}\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == \"object\":\n",
    "            metric_list = df[column].explode().tolist()\n",
    "            metric_dict = {}\n",
    "            for metric in metric_list:\n",
    "                if metric[0] in metric_dict:\n",
    "                    metric_dict[metric[0]].append(metric[1])\n",
    "                else:\n",
    "                    metric_dict[metric[0]] = [metric[1]]\n",
    "            for key, values in metric_dict.items():\n",
    "                new_col_name = f\"{column} {key}\"\n",
    "                metric_averages[new_col_name] = round(sum(values) / len(values), 2)\n",
    "        else:\n",
    "            metric_averages[column] = round(df[column].mean(), 2)\n",
    "    return metric_averages\n",
    "\n",
    "    \n",
    "# split up the name into categories\n",
    "def split_name(name):\n",
    "    # dataset name is everything before \"-m\"\n",
    "    dataset = name.split(\"-m\")[0]\n",
    "    # model name is everything between \"-m\" and \"-e\"\n",
    "    model = name.split(\"-m\")[1].split(\"-e\")[0]\n",
    "    # embedding name is everything after \"-e\"\n",
    "    embedding = name.split(\"-e\")[1]\n",
    "    # remove trailing underscores\n",
    "    dataset = dataset[:-1]\n",
    "    model = model[1:-1]\n",
    "    embedding = embedding[1:]\n",
    "    return dataset, model, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for df in dataframes:\n",
    "    dataset, model, embedding = split_name(df)\n",
    "    result_list.append({**{'dataset': dataset}, **{'model': model}, **{'fe': embedding}, **average_metrics(dataframes[df])})\n",
    "\n",
    "df = pd.DataFrame(result_list)\n",
    "\n",
    "# make into a json file\n",
    "with open(\"results.json\", \"w\") as json_file:\n",
    "    json.dump(result_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leftover vars\n",
    "del f, folder, folder_path, json_data, json_file, json_file_path, json_files, metric_data, metrics, root_dir, model, embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df remove Walker_2018 dataset\n",
    "df = df[df.dataset != 'Walker_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['fe','model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['Work Saved over Sampling 0.95'], by=['fe','model'], figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['Work Saved over Sampling 0.95'], by=['model'], figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['Work Saved over Sampling 0.95'], by=['fe'], figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by wss 0.95\n",
    "df.groupby(\"dataset\").mean().sort_values(by=\"Work Saved over Sampling 0.95\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
